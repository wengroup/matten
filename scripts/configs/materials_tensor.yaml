seed_everything: 35
log_level: info

data:
  root: ../datasets/
  r_cut: 5.0
  trainset_filename: example_crystal_elasticity_tensor_n100.json
  valset_filename: example_crystal_elasticity_tensor_n100.json
  testset_filename: example_crystal_elasticity_tensor_n100.json
  reuse: false
  loader_kwargs:
    batch_size: 32
    shuffle: true

model:
  ##########
  # embedding
  ##########

  # atom species embedding
  species_embedding_dim: 16

  # spherical harmonics embedding of edge direction
  irreps_edge_sh: 0e + 1o + 2e + 3o + 4e

  # radial edge distance embedding
  radial_basis_type: bessel
  num_radial_basis: 8
  radial_basis_start: 0.
  radial_basis_end: 5.

  ##########
  # message passing conv layers
  ##########
  num_layers: 3

  # radial network
  invariant_layers: 2 # number of radial layers
  invariant_neurons: 32 # number of hidden neurons in radial function

  # Average number of neighbors used for normalization. Options:
  # 1. `auto` to determine it automatically, by setting it to average number
  #    of neighbors of the training set
  # 2. float or int provided here.
  # 3. `null` to not use it
  average_num_neighbors: auto

  # point convolution
  conv_layer_irreps: 32x0o+32x0e + 16x1o+16x1e + 4x2o+4x2e + 2x3o+2x3e + 2x4e
  nonlinearity_type: gate
  normalization: batch
  resnet: true

  ##########
  # output
  ##########

  conv_to_output_hidden_irreps_out: 16x0e + 2x2e + 4e

  # output_format and output_formula should be used together.
  # - output_format (can be `irreps` or `cartesian`) determines what the loss
  #   function will be on (either on the irreps space or the cartesian space).
  # - output_formula gives what the cartesian formula of the tensor is.
  #   For example, ijkl=jikl=klij specifies a forth-rank elasticity tensor.
  output_format: irreps
  output_formula: ijkl=jikl=klij

  # pooling node feats to graph feats
  reduce: mean

trainer:
  max_epochs: 10 # number of maximum training epochs
  num_nodes: 1
  accelerator: cpu
  devices: 1

  callbacks:
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        monitor: val/score
        mode: min
        save_top_k: 3
        save_last: true
        verbose: false
    - class_path: pytorch_lightning.callbacks.EarlyStopping
      init_args:
        monitor: val/score
        mode: min
        patience: 150
        min_delta: 0
        verbose: true
    - class_path: pytorch_lightning.callbacks.ModelSummary
      init_args:
        max_depth: -1

  #logger:
  #  class_path: pytorch_lightning.loggers.wandb.WandbLogger
  #  init_args:
  #    save_dir: matten_logs
  #    project: matten_proj

optimizer:
  class_path: torch.optim.Adam
  init_args:
    lr: 0.01
    weight_decay: 0.00001

lr_scheduler:
  class_path: torch.optim.lr_scheduler.ReduceLROnPlateau
  init_args:
    mode: min
    factor: 0.5
    patience: 50
    verbose: true
